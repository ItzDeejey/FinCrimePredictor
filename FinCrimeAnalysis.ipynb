{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ItzDeejey/FinCrimePredictor/blob/main/FinCrimeAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EWpsq2xsCGU",
        "outputId": "fbd774d8-7cd4-4a16-ac98-05106038da5b"
      },
      "id": "2EWpsq2xsCGU",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "\n",
        "# Read only the first 100 rows from the large CSV\n",
        "df_preview = pl.read_csv(\"/content/drive/MyDrive/Colab Notebooks/financial_fraud_detection_dataset.csv\", n_rows=100)\n",
        "\n",
        "# Write those rows to a new CSV file\n",
        "df_preview.write_csv(\"preview.csv\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"preview.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "73gxvbuB32eB",
        "outputId": "b7d7aa16-1cb3-4d65-cf5a-59769a853256"
      },
      "id": "73gxvbuB32eB",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_25373a60-31d8-4a30-a287-c54291a082cc\", \"preview.csv\", 14511)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import polars as pl\n",
        "from IPython.display import FileLink\n",
        "\n",
        "# Create a lazy frame from the CSV (this does not load the whole file into memory)\n",
        "lf = pl.scan_csv(\"/content/drive/MyDrive/Colab Notebooks/financial_fraud_detection_dataset.csv\")\n",
        "\n",
        "# Filter for rows where 'time_since_last_transaction' is not null\n",
        "# and limit the result to a preview of 100 rows\n",
        "lf_filtered = lf.filter(pl.col(\"time_since_last_transaction\").is_not_null()).limit(100)\n",
        "\n",
        "# Collect the filtered preview into a DataFrame\n",
        "df_preview_non_null = lf_filtered.collect()\n",
        "\n",
        "# Write the preview to a new CSV file\n",
        "df_preview_non_null.write_csv(\"preview_non_null.csv\")\n",
        "\n",
        "# Create a clickable download link (only works in a Jupyter Notebook)\n",
        "files.download(\"preview_non_null.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JWEr_QA3pTRZ",
        "outputId": "c0d0be3d-c46f-409d-ca1c-d5cf4f27e786"
      },
      "id": "JWEr_QA3pTRZ",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6decd6da-6e4f-4df8-af05-0c23d73b17a4\", \"preview_non_null.csv\", 16316)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "-NwC7DNt1ylF"
      },
      "id": "-NwC7DNt1ylF"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e6946321-916c-43ad-b3af-e2892fa81861",
      "metadata": {
        "id": "e6946321-916c-43ad-b3af-e2892fa81861"
      },
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "\n",
        "# Use scan_csv for a lazy/streaming read\n",
        "lf = pl.scan_csv(\"/content/drive/MyDrive/Colab Notebooks/financial_fraud_detection_dataset.csv\")\n",
        "\n",
        "# Apply your preprocessing transformations:\n",
        "def parse_timestamp(col):\n",
        "    return pl.when(pl.col(col).str.contains(r\"\\.\\d+$\")) \\\n",
        "             .then(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S%.f\", strict=False)) \\\n",
        "             .otherwise(pl.col(col).str.strptime(pl.Datetime, \"%Y-%m-%dT%H:%M:%S\", strict=False))\n",
        "\n",
        "num_cols = ['spending_deviation_score', 'velocity_score', 'geo_anomaly_score', 'amount', 'time_since_last_transaction']\n",
        "cat_cols = ['transaction_type', 'merchant_category', 'location', 'device_used', 'payment_channel'] #, 'fraud_type'\n",
        "text_cols = ['sender_account', 'receiver_account', 'ip_address', 'device_hash']\n",
        "\n",
        "lf = (\n",
        "    lf\n",
        "    #.drop('time_since_last_transaction')\n",
        "    .unique()\n",
        "    .with_columns([\n",
        "         pl.when((pl.col(\"is_fraud\") == False) & (pl.col(\"fraud_type\").is_null() | (pl.col(\"fraud_type\") == \"\")))\n",
        "           .then(pl.lit(\"No Fraud\"))\n",
        "           .otherwise(pl.col(\"fraud_type\"))\n",
        "           .alias(\"fraud_type\")\n",
        "    ])  # <== Properly close the first with_columns call here.\n",
        "    .with_columns([\n",
        "         *[pl.col(col).cast(pl.Utf8) for col in text_cols],\n",
        "         parse_timestamp(\"timestamp\").alias(\"timestamp\"),\n",
        "         *[pl.col(col).cast(pl.Float64) for col in num_cols],\n",
        "         *[pl.col(col).cast(pl.Categorical) for col in cat_cols]\n",
        "    ])\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lf.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLdRPRndplk0",
        "outputId": "b0f5e3bd-a0af-4a6c-ce03-4a96048d045f"
      },
      "id": "aLdRPRndplk0",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2177650233.py:1: PerformanceWarning: Resolving the schema of a LazyFrame is a potentially expensive operation. Use `LazyFrame.collect_schema()` to get the schema without this warning.\n",
            "  lf.schema\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Schema([('transaction_id', String),\n",
              "        ('timestamp', Datetime(time_unit='us', time_zone=None)),\n",
              "        ('sender_account', String),\n",
              "        ('receiver_account', String),\n",
              "        ('amount', Float64),\n",
              "        ('transaction_type', Categorical(ordering='physical')),\n",
              "        ('merchant_category', Categorical(ordering='physical')),\n",
              "        ('location', Categorical(ordering='physical')),\n",
              "        ('device_used', Categorical(ordering='physical')),\n",
              "        ('is_fraud', Boolean),\n",
              "        ('fraud_type', String),\n",
              "        ('time_since_last_transaction', Float64),\n",
              "        ('spending_deviation_score', Float64),\n",
              "        ('velocity_score', Float64),\n",
              "        ('geo_anomaly_score', Float64),\n",
              "        ('payment_channel', Categorical(ordering='physical')),\n",
              "        ('ip_address', String),\n",
              "        ('device_hash', String)])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "58a9da2b-2802-47bc-9e5d-88e4028184c1",
      "metadata": {
        "id": "58a9da2b-2802-47bc-9e5d-88e4028184c1"
      },
      "outputs": [],
      "source": [
        "# Convert categorical columns to integer codes to use as features later\n",
        "for col in cat_cols:\n",
        "    lf = lf.with_columns([pl.col(col).to_physical().cast(pl.Int64).alias(col + \"_code\") for col in cat_cols])\n",
        "\n",
        "# Define final feature columns:\n",
        "final_cat_cols = [col + \"_code\" for col in cat_cols]\n",
        "final_features = num_cols + final_cat_cols\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect the entire preprocessed dataset into a DataFrame first.\n",
        "df_polars_full = lf.collect()\n"
      ],
      "metadata": {
        "id": "m_atW2lk0Nfu"
      },
      "id": "m_atW2lk0Nfu",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = df_polars_full.filter(pl.col(\"time_since_last_transaction\").is_not_null())\n",
        "not_null_count = len(filtered)\n",
        "\n",
        "print(\"Non-null count for 'time_since_last_transaction':\", not_null_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja8UlDOAB-UM",
        "outputId": "b73f1c97-7caa-4a29-aebb-08bafa40591b"
      },
      "id": "ja8UlDOAB-UM",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-null count for 'time_since_last_transaction': 4103487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b89e7UxioA4O",
        "outputId": "2765f953-ac9c-4574-b89c-f7550a1bd75c"
      },
      "id": "b89e7UxioA4O",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['spending_deviation_score',\n",
              " 'velocity_score',\n",
              " 'geo_anomaly_score',\n",
              " 'amount',\n",
              " 'time_since_last_transaction',\n",
              " 'transaction_type_code',\n",
              " 'merchant_category_code',\n",
              " 'location_code',\n",
              " 'device_used_code',\n",
              " 'payment_channel_code']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow as pa\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import polars as pl # Ensure polars is imported if it wasn't already in this cell\n",
        "\n",
        "# Split the lazy frame into training and test sets AFTER preprocessing\n",
        "test_sample_fraction = 0.4 # Define the fraction of data for testing\n",
        "\n",
        "# Instead of sampling the LazyFrame, we will collect the full preprocessed data\n",
        "# and then split the collected DataFrame.\n",
        "# However, since the goal is streaming training, we will collect the full\n",
        "# dataset first, shuffle it, and then split.\n",
        "\n",
        "# Collect the entire preprocessed dataset into a DataFrame first.\n",
        "# For potentially large datasets, consider collecting the full dataset in batches\n",
        "# if memory is a concern, and then shuffling within batches or using a different\n",
        "# splitting strategy if a full shuffle is not feasible.\n",
        "# Given the use of SGDClassifier for incremental training, collecting the full\n",
        "# dataset first to split might be okay, as you're still processing it in batches\n",
        "# during training. If the dataset is too large to fit in memory even after\n",
        "# collecting, a different splitting strategy might be needed (e.g., hashing\n",
        "# based split on a key).\n",
        "# For this fix, we assume the collected preprocessed data can fit into memory for splitting.\n",
        "df_polars_full = lf.collect()\n",
        "\n",
        "# Shuffle the collected DataFrame for a random split\n",
        "df_polars_full = df_polars_full.sample(fraction=1.0, with_replacement=False, seed=42) # Shuffle\n",
        "\n",
        "# Calculate the split point\n",
        "num_rows_full = df_polars_full.shape[0]\n",
        "split_point = int(num_rows_full * (1 - test_sample_fraction))\n",
        "\n",
        "# Split the shuffled DataFrame into training and test sets\n",
        "df_polars_train = df_polars_full.head(split_point)\n",
        "df_polars_test = df_polars_full.tail(num_rows_full - split_point)\n",
        "\n",
        "# Drop rows with any NaN values in the final_features columns from the training data\n",
        "df_polars_train_cleaned = df_polars_train.drop_nulls(final_features)\n",
        "# Drop rows with any NaN values in the final_features columns from the test data\n",
        "df_polars_test_cleaned = df_polars_test.drop_nulls(final_features)\n",
        "\n",
        "\n",
        "# Convert the cleaned training DataFrame to an Arrow Table for batch processing\n",
        "arrow_table = df_polars_train_cleaned.to_arrow()\n",
        "\n",
        "# Set your chunk (batch) size:\n",
        "batch_size = 5000\n",
        "\n",
        "# Calculate the number of batches for the training data\n",
        "num_rows_train_cleaned = arrow_table.num_rows\n",
        "num_batches = (num_rows_train_cleaned + batch_size - 1) // batch_size # Ceiling division\n",
        "\n",
        "# Initialize your incremental model.\n",
        "model = SGDClassifier(loss=\"log_loss\", random_state=42)\n",
        "classes = np.array([0, 1])\n",
        "first_batch = True\n",
        "\n",
        "# Train on each batch by slicing the Arrow table.\n",
        "# Iterate through the calculated number of batches\n",
        "for i in range(num_batches):\n",
        "    # Define the start and end row index for the current batch\n",
        "    start_row = i * batch_size\n",
        "    end_row = min((i + 1) * batch_size, num_rows_train_cleaned) # Use num_rows_train_cleaned here\n",
        "\n",
        "    # Slice the arrow_table to get the current batch (as a Table)\n",
        "    batch_arrow_table = arrow_table.slice(offset=start_row, length=end_row - start_row)\n",
        "\n",
        "    # Convert the Arrow Table batch to an Arrow RecordBatch\n",
        "    # We expect a single RecordBatch from the slice in this case\n",
        "    # Check if there are any batches before accessing index 0\n",
        "    batches = batch_arrow_table.to_batches()\n",
        "    if batches:\n",
        "        batch = batches[0]\n",
        "    else:\n",
        "        continue # Skip if batch is empty\n",
        "\n",
        "    # Convert each Arrow RecordBatch to a Polars DataFrame:\n",
        "    batch_df = pl.from_arrow(batch)\n",
        "\n",
        "    # Extract feature matrix (X) and target (y):\n",
        "    # final_features contains your numeric columns and the encoded categorical columns.\n",
        "    X_chunk = batch_df.select(final_features).to_numpy()\n",
        "    y_chunk = batch_df.select(\"is_fraud\").to_numpy().ravel()\n",
        "\n",
        "    if first_batch:\n",
        "        model.partial_fit(X_chunk, y_chunk, classes=classes)\n",
        "        first_batch = False\n",
        "    else:\n",
        "        model.partial_fit(X_chunk, y_chunk)\n",
        "\n",
        "print(\"Incremental training complete!\")\n",
        "\n",
        "# Collect test data (already collected as df_polars_test)\n",
        "# df_test_polars = test_lf.collect(streaming=True) # Remove this line\n",
        "X_test = df_polars_test_cleaned.select(final_features).to_numpy()\n",
        "y_test = df_polars_test_cleaned.select(\"is_fraud\").to_numpy().ravel()\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test Evaluation:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaEJF4SYvZ5A",
        "outputId": "83717208-7553-4a7d-8831-5620613123cf"
      },
      "id": "TaEJF4SYvZ5A",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Incremental training complete!\n",
            "Test Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.96      0.95      0.95   1569843\n",
            "        True       0.04      0.05      0.05     71567\n",
            "\n",
            "    accuracy                           0.91   1641410\n",
            "   macro avg       0.50      0.50      0.50   1641410\n",
            "weighted avg       0.92      0.91      0.91   1641410\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform value_counts directly on the collected DataFrame\n",
        "df_counts = df_polars_full[\"is_fraud\"].value_counts()\n",
        "\n",
        "# The result of value_counts on a Series is a DataFrame with columns 'is_fraud' and 'counts'\n",
        "# Now we can directly calculate the ratio\n",
        "total = df_counts[\"count\"].sum()\n",
        "\n",
        "# Calculate the ratio and add it as a new column\n",
        "df = df_counts.with_columns(\n",
        "    (pl.col(\"count\") / total).alias(\"ratio\")\n",
        ")\n",
        "\n",
        "display(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "iQUW0GIHsC8B",
        "outputId": "02e6cdef-e816-4457-8ed6-dc409814b59a"
      },
      "id": "iQUW0GIHsC8B",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "shape: (2, 3)\n",
              "┌──────────┬─────────┬───────────┐\n",
              "│ is_fraud ┆ count   ┆ ratio     │\n",
              "│ ---      ┆ ---     ┆ ---       │\n",
              "│ bool     ┆ u32     ┆ f64       │\n",
              "╞══════════╪═════════╪═══════════╡\n",
              "│ true     ┆ 179553  ┆ 0.0359106 │\n",
              "│ false    ┆ 4820447 ┆ 0.9640894 │\n",
              "└──────────┴─────────┴───────────┘"
            ],
            "text/html": [
              "<div><style>\n",
              ".dataframe > thead > tr,\n",
              ".dataframe > tbody > tr {\n",
              "  text-align: right;\n",
              "  white-space: pre-wrap;\n",
              "}\n",
              "</style>\n",
              "<small>shape: (2, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>is_fraud</th><th>count</th><th>ratio</th></tr><tr><td>bool</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>true</td><td>179553</td><td>0.0359106</td></tr><tr><td>false</td><td>4820447</td><td>0.9640894</td></tr></tbody></table></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_chunk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSzIhq5lny82",
        "outputId": "5a408bdc-a704-4d77-d35e-0b43f855c7bd"
      },
      "id": "OSzIhq5lny82",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.1 ,  6.  ,  0.91, ...,  3.  ,  0.  ,  2.  ],\n",
              "       [-0.4 , 10.  ,  0.89, ...,  4.  ,  3.  ,  1.  ],\n",
              "       [ 0.02,  1.  ,  0.  , ...,  7.  ,  2.  ,  3.  ],\n",
              "       ...,\n",
              "       [ 1.4 ,  5.  ,  0.78, ...,  3.  ,  3.  ,  1.  ],\n",
              "       [ 0.13, 19.  ,  0.25, ...,  4.  ,  1.  ,  0.  ],\n",
              "       [ 2.61, 19.  ,  0.42, ...,  5.  ,  1.  ,  1.  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect test data\n",
        "# df_test_polars = test_lf.collect(streaming=True) # This line caused the error, remove it\n",
        "# Use the already created df_polars_test_cleaned DataFrame\n",
        "X_test = df_polars_test_cleaned.select(final_features).to_numpy()\n",
        "y_test = df_polars_test_cleaned.select(\"is_fraud\").to_numpy().ravel()\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test Evaluation:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JC77ElV3jhu",
        "outputId": "7637a53d-7620-438c-f536-ce28eda2e897"
      },
      "id": "0JC77ElV3jhu",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Evaluation:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.96      0.95      0.95   1569843\n",
            "        True       0.04      0.05      0.05     71567\n",
            "\n",
            "    accuracy                           0.91   1641410\n",
            "   macro avg       0.50      0.50      0.50   1641410\n",
            "weighted avg       0.92      0.91      0.91   1641410\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}